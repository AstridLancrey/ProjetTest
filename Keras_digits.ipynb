{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement du dataset mnist de keras pour entraîner un FNN (Fully connected Neural Network) à la reconnaissance d'images\n",
    "\n",
    "Le dataset mnist de keras contient un **training_dataset** et un **testing_dataset** de respectivement **60 000** et **10 000** images d'un chiffre dessiné à la main. Chaque image est décomposée en un quadrillage de 28 x 28 \"cases\", chaque case étant associée à une valeur de pixel. **x_train** est donc une **matrice 3D** de shape: **60 000 x 28 x 28**. Chaque indice de zero à 59 000 de x_train correspond ainsi à une matrice 2D de 28 listes de 28 valeurs, chaque valeur correspondant à un nombre de pixels compris entre environ 0 et 250. Pour chaque indice de 0 à 59 000 la valeur de **y_train** correspondante est **le label/la class** (et l'output) de l'image: **soit un chiffre de 1 à 10**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data() # chargement du jeu de données\n",
    "\n",
    "# (x_train, y_train) = training set : jeu de données utilisé pour la phase d'apprentissage du cnn\n",
    "# (x_test, y_test) = testing set : jeu de données utilisé pour la phase de test du cnn (vérification de sa fiabilité)\n",
    "\n",
    "\n",
    "## Visualisation du jeu de données:\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(x_train.shape)\n",
    "\n",
    "print(x_train[0])\n",
    "print(y_train[0])\n",
    "plt.imshow(x_train[0], cmap = plt.cm.binary)\n",
    "plt.show()\n",
    "\n",
    "print(x_train[59000])\n",
    "print(y_train[59000])\n",
    "plt.imshow(x_train[59000], cmap = plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalisation des données inputs entre 0 et 1 (vivement conseillé)\n",
    "\n",
    "x_train = tf.keras.utils.normalize(x_train, axis=1)\n",
    "x_test = tf.keras.utils.normalize(x_test, axis=1)\n",
    "\n",
    "## Visualisation du jeu de données après normalisation:\n",
    "\n",
    "print(x_train[0])\n",
    "print(y_train[0])\n",
    "plt.imshow(x_train[0], cmap = plt.cm.binary)\n",
    "plt.show()\n",
    "\n",
    "print(x_train[59000])\n",
    "print(y_train[59000])\n",
    "plt.imshow(x_train[59000], cmap = plt.cm.binary)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Définition du modèle (= architecture du cnn) avec keras ##\n",
    "\n",
    "model = tf.keras.models.Sequential() # modèle sequentiel\n",
    "model.add(tf.keras.layers.Flatten()) # input layer --> reshaped à une dimension (et non plus 2D (matrice 28 x 28))\n",
    "model.add(tf.keras.layers.Dense(128, activation = tf.nn.relu)) # 1st inner layer definition : 128 neurones \n",
    "model.add(tf.keras.layers.Dense(128, activation = tf.nn.relu)) # 2nd inner layer definition : 128 neurones\n",
    "# dense par opposition à flatten : plusieurs dimensions pour l'input reçu \n",
    "# Par défaut c'est la fonction d'activation ReLu (si x < 0, y = 0; si x >= 0, y = x) qui est utilisé pour les inner layers\n",
    "model.add(tf.keras.layers.Dense(10, activation = tf.nn.softmax)) # outpout layer\n",
    "# Nombre de neurones = nombre de classes/outputs possibles : ici 10\n",
    "# Pour un cnn utilisé pour faire de la classification (= probability distribution) c'est la fontion softmax qu'on utilise comme fonction d'activation de l'oupout layer\n",
    "\n",
    "## définition des paramètres de training du modèle:\n",
    "model.compile(optimizer = 'adam', # adam est l'optimizer par défaut\n",
    "              loss = 'sparse_categorical_crossentropy', # loss aurait pu être binary pour une classification à 2 classes\n",
    "              metrics = ['accuracy'])\n",
    "## Training:\n",
    "model.fit(x_train, y_train, epochs = 3)\n",
    "\n",
    "## \" Epoch is just a \"full pass\" through the entire training dataset. \n",
    "## So if we train on 1 epoch, the neural network sees each unique sample once. \n",
    "## 3 epochs means it passed over our data set 3 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_acc = model.evaluate(x_test, y_test)\n",
    "print(val_loss, val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('digits_pictures')\n",
    "new_model = tf.keras.models.load_model('digits_pictures')\n",
    "predictions = new_model.predict([x_test])\n",
    "print(predictions) #imprime une distribution de probabilités pour les 10 classes pour les 10 000 images du dataset test\n",
    "print(predictions[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print (np.argmax(predictions[0])) # permet d'imprimer la valeur y_test[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
